= Spark json config

== Meta fields

|===
|Name           |State                      |Description
|`name`         |required                   |Name of the package
|`version`      |required for publishing    |Current version of the package
|`description`  |recommended for publishing |Short description of the package
|`copyright`    |recommended for publishing |Copyright notice
|`license`      |required for publishing    |SPDX license identifier
|`keywords`     |recommended for publishing |List of keywords to enhance discoverability
|`homepage`     |required for publishing    |Homepage of the project
|`bugs`         |required for publishing    |Page to report bugs / issues to
|`authors`      |recommended for publishing |Authors of the package
|===

=== Authors

The authors field is an array of of objects:
```json
{
    "name": "",
    "email": "",
    "homepage": "",
    "social": ""
}
```
All fields except `name` are optional.

== Build process fields

|===
|Name                       |State      |Description
|`dependencies`             |optional   |Specifies the dependencies for this package
|`dependency_management`    |optional   |Specifies informations about dependecy management
|`unpack_cmds`              |optional   |Specifies commands to unpack url packages
|===

=== Dependencies

Dependencies need to be specied as an object of key-value pairs, where the key is the package name and the value is either a string (equivalent to `{ "version": x }`) or a object:

[source,json]
----
{
    // the version specification
    "version": "",
}
----

Version field:

- `v`: exact version
- `&gt;v`, `&lt;v`, `&gt;=v`, `&lt;=v`: exactly what you think it is
- `1.2.x`: matches `1.2.0`, `1.2.1` and so on but not `1.3.0`
- `v1 - v2`: range, same as `&gt;=v1 &lt;=v2`
- `v || v`: checks if either of both matches; can only use version specifications

Additionally it supports:

- `*`: any
- `latest`: latest version
- `https://`: url to download package from; see below
- `file:xxx`: local path; see below
- `git...`: git repository; see below

==== URL dependencies

When specifing an url as dependency sparks downloads the file and:

- checks if it is an commonly known archive such as `.zip` or `.tar` (tar also supports common known compressions)
- if it is not know, it checks if a entry in `unpack_cmds` is available and uses that to unpack the file
- spark expects to get a spark package after all is done

==== Local paths

Local paths can either refer to files or directories: in case of a file, the same process as for URL dependencies is done, and `unpack_cmds` are used to unpack the file. If it is a directory, spark simply expects to find a spark package.

==== Git repository

Format:
----
<proto>://[<usr>[:<pass>]@]<host>[:<port>][/<path>][#<commit-ish> | #semver:<semver>]
----

`proto` is the protocol to use; needs to be one of these: `git`, `git+ssh`, `git+http`, `git+https`, `git+file`.

If `#<commit-ish>` is supplied, that commit (or ref) is used for the clone.
When instead `#semver:<semver>` is used, spark tries to match tags based on the normal semver matching rules like in "normal" version specification (see above).

=== Dependency management

[source,json]
----
{
    "dependency_management": {
        // Used to cache all dependencies inside the project folder
        // can be used to create a project-wide 'vendor' folder.
        //
        // By default no local cache is created.
        "local_cache": "",

        // Used to specify if binary artifacts should be downloaded & used; default is false.
        "binary_artifacts": true,

        // Configuration about specific package repositories.
        "repositories": [
            {
                // Url of the repo
                "url": "",

                // Used to specify if binary artifacts of this repo should be downloaded & used; default is false.
                "binary_artifacts": true,

                // Used to cache all dependencies of this repository inside the project folder
                // can be used to create a per repository 'vendor' folder.
                //
                // By default no local cache is created.
                "local_cache": "",

                // Specifies a set of match regex's for package names, which should be served from this repository instead.
                "matchers": [],
            }
        ],
    }
}
----

Url can on of the following:
- `git:...` download repo-content from a git repo
- `http://` and `https://` are normal spark repositories

=== Unpack commands

The unpack commands configuration is used when spark needs to unpack downloaded or local files.

[source,json]
----
{
    "unpack_cmds": {
        ".tar.xz": "tar -x",
        ".my.pack": "myunpack --input %SRC --output %DEST"
    }
}
----

The key has the following format:

- if it starts with `.`, it checks for fileextension only
- all others will be used as a regex that will be matched onto the filename; in this case you can use `$XXX` to access groups from the match inside the command; Note however that `$SRC` and `$DEST` have special meanings!

The source file (package archive) is given to the process via stdin; however if your binary dosnt support that, you can use `$SRC` in the command to be given a (possible temporary) filename to access the file. This then disables the stdin behaviour.

The process's cwd is set to the destination folder; however if you need it from the commandline you also can use `$DEST`. Note that this dosnt disable the cwd behaviour.
